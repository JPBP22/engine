{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARP (Weighted Approximate-Rank Pairwise) Theoretical Explanation\n",
    "\n",
    "WARP (Weighted Approximate-Rank Pairwise) loss is a loss function used in collaborative filtering models for ranking tasks, such as recommendation systems. The main idea behind WARP is to maximize the rank of the positive item relative to the negative items by sampling the negative items in a way that is likely to produce informative updates to the model parameters.\n",
    "\n",
    "The loss function is defined as follows:\n",
    "\n",
    "$L_{i,j} = sum_{j'=1}^{j-1} max(0, 1 - s(i,j) + s(i,j'))$\n",
    "\n",
    "where:\n",
    "\n",
    "- i is the user\n",
    "- j is the positive item\n",
    "- j' is a negative item\n",
    "- s(i,j) is the predicted score of item j for user i\n",
    "\n",
    "The sampling is done in a way that gives preference to the negative items that are the hardest to rank higher than the positive item, i.e., the ones that have the highest predicted score.\n",
    "\n",
    "Mathematically, WARP is based on the idea of stochastic gradient descent, a popular optimization algorithm used in machine learning. At each step of the algorithm, WARP updates the weights of the positive and negative examples based on the following formula:\n",
    "\n",
    "w = w + η(α - σ)\n",
    "\n",
    "where w is the weight of the example, η is the learning rate, α is the maximum margin (or difference in rank) that the algorithm is trying to achieve, and σ is the current margin between the positive and negative examples.\n",
    "\n",
    "The algorithm continues to iterate until a pre-specified number of iterations is reached or the desired performance metric is achieved.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPR loss model Theoretical Explanation\n",
    "\n",
    "BPR (Bayesian Personalized Ranking) loss is another loss function commonly used in matrix factorization-based collaborative filtering models for ranking tasks, such as recommendation systems. The main idea behind BPR is to maximize the pairwise ranking of items based on users' implicit feedback, such as clicks or purchases, by modeling the preference of each user for the items they have interacted with.\n",
    "\n",
    "$L_{i,j,j'} = -ln(sigma(s(i,j) - s(i,j')))$\n",
    "\n",
    "where:\n",
    "\n",
    "- i is the user\n",
    "- j is a positive item\n",
    "- j' is a negative item\n",
    "- s(i,j) is the predicted score of item j for user i\n",
    "- sigma is the sigmoid function\n",
    "\n",
    "The loss function encourages the model to assign a higher score to the positive item $j$ than the negative item $j'$ for user $i$.\n",
    "\n",
    "Mathematically, BPR is based on the idea of maximum posterior estimation, which involves maximizing the posterior probability of the latent features given the observed data. The posterior probability is proportional to the product of the prior probability of the features and the likelihood of the data given the features.\n",
    "\n",
    "In the case of BPR, the prior probability of the features is assumed to follow a Gaussian distribution, and the likelihood of the data is modeled using a pairwise logistic function. The pairwise logistic function is used to estimate the probability that a user prefers one item over another, given their latent feature representations.\n",
    "\n",
    "The BPR algorithm then optimizes the likelihood of the observed data by minimizing the negative log-likelihood using stochastic gradient descent. The gradient of the negative log-likelihood is computed using the chain rule of calculus, and the parameters of the model are updated iteratively.\n",
    "\n",
    "The code we provided is using the BPR loss function in a matrix factorization model with 30 latent factors. The run_model function is fitting the model to the training data train and specifying various parameters such as the number of epochs and the number of jobs to use for parallel processing.\n",
    "\n",
    "After fitting the model, the code computes the precision and AUC scores on the training and test sets using the precision_at_k and auc_score functions, respectively. These metrics are commonly used to evaluate the performance of recommendation systems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "SVD (Singular Value Decomposition) is a widely used matrix factorization technique that decomposes a matrix into three parts: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. SVD is commonly used for dimensionality reduction, data compression, and collaborative filtering.\n",
    "\n",
    "The mathematical foundation of SVD is based on linear algebra and matrix decomposition. Given a matrix A of size m x n, SVD factorizes A into the following form:\n",
    "\n",
    "A = U Σ V^T\n",
    "\n",
    "where U is an m x m orthogonal matrix of left singular vectors, Σ is an m x n diagonal matrix of singular values, and V is an n x n orthogonal matrix of right singular vectors.\n",
    "\n",
    "The singular values in Σ represent the square roots of the eigenvalues of A^T A or A A^T, and they are sorted in decreasing order. The left singular vectors in U and right singular vectors in V represent the directions of maximum variance in the row and column spaces of A, respectively.\n",
    "\n",
    "SVD is useful for dimensionality reduction because it allows us to approximate the original matrix A by truncating the singular value matrix Σ to a smaller size. Specifically, if we keep only the k largest singular values in Σ, we can obtain a low-rank approximation of A:\n",
    "\n",
    "A ≈ U_k Σ_k V_k^T\n",
    "\n",
    "where U_k, Σ_k, and V_k are the first k columns of U, the first k rows and columns of Σ, and the first k rows of V, respectively.\n",
    "\n",
    "This low-rank approximation can be used to compress the data, reduce noise, and improve the efficiency of subsequent algorithms that operate on the data.\n",
    "\n",
    "In collaborative filtering, SVD is used to learn the latent features of users and items by factorizing the user-item matrix into the three parts. The resulting low-rank approximation can then be used to make personalized recommendations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
